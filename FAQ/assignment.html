인공지능 기초를 위한 FAQ

14번 ~ 27번, 33번 제외, 19, 20 포함

1. 인공지능에서 지능에 해당하는 기능은 무엇인가?

학습하고 논리적으로 추론하는 능력
패턴을 인식하고 주어진 상황을 해석하는 능력
상황을 단순화시켜 문제의 본질 분석 능력
다양하게 만나는 문제들의 해결 능력

인공지능(AI)에서 지능에 해당하는 기능은 인간의 인지 능력을 모방하거나 구현하는 다양한 기능
학습 (Learning)
데이터를 분석하고 패턴을 학습하여 새로운 정보를 습득하는 능력
지도학습, 비지도학습, 강화학습 등의 방식이 존재
예: 머신러닝 모델이 사진을 보고 고양이와 개를 구별하는 능력

추론 (Reasoning)
주어진 정보나 규칙을 바탕으로 논리적인 결론을 도출하는 능력
연역적 추론(규칙 → 개별 사례)과 귀납적 추론(사례 → 일반 규칙)으로 구분
예: 체스 AI가 여러 수를 고려하여 최적의 수를 결정하는 과정

문제 해결 (Problem Solving)
주어진 문제를 해결하기 위한 최적의 방법을 찾는 능력
탐색 알고리즘, 최적화 기법 등을 활용
예: 네비게이션 시스템이 최단 경로를 찾는 기능

자연어 처리 (Natural Language Processing, NLP)
인간의 언어를 이해하고 생성하는 능력
텍스트 분석, 기계 번역, 음성 인식 등이 포함
예: 챗봇이 질문에 답변하는 기능

지각 (Perception)
시각, 청각 등 감각 데이터를 해석하는 능력
컴퓨터 비전, 음성 인식 기술 등이 포함
예: 자율주행차가 교통 신호를 인식하는 기능

계획 및 의사결정 (Planning & Decision Making)
목표를 설정하고 최적의 행동을 결정하는 능력
강화학습, 의사결정 트리 등을 활용
예: 로봇이 장애물을 피해 목적지까지 이동하는 기능

2. 인공지능의 종류 3가지에 대해서 설명하시오 (지도학습, 반지도학습, 강화학습)

 - 지도 학습 : 데이터를 학습시킬 때 input data의 정답과 함께 학습시켜 new input이 생길 때 output을 예측하는 학습 방법이다.
   지도 학습 모델은 입력과 정답의 관계를 학습하여 새로운 데이터에 대한 예측을 할 수 있다.(label있음)

 - 비지도 학습 : 지도 학습과 달리 input data에 대한 정답을 알려주지 않고 데이터만 제공하여 학습하는 방법이다.
   비지도 학습 모델은 스스로 데이터의 구조나 패턴을 찾아낸다. 데이터의 특성을 분석하여 유사한 그룹을 나타내거나, 중요한 특징을 추출한다.(label없음)

 - 강화 학습 : 주어진 입력에 대해 대응하는 보상을 주어 보상을 기준으로 최적의 행동을 학습하는 방법이다. 로봇, 게임, 네이게이션에 응용된다.(label 없음)

 - 반지도 학습 : 일부 데이터에만 정답이 있고, 나머지 대부분의 데이터에는 정답이 없는 상태에서 학습하는 방법이다.(label 일부분 있음)
  정답이 있는 데이터를 통해 기본적인 분류 기준을 학습한 후, 정답이 없는 데이터를 분석해 데이터의 구조나 패턴을 파악하고,
  이를 바탕으로 더 정확한 예측이나 분류를 수행한다.

3. 전통적인 프로그래밍 방법과 인공지능 프로그램의 차이점은 무엇인가?

 - 전통적인 프로그래밍 : Data를 입력받고 프로그래머가 임의의 규칙을 만들어 설정하면 이를 적용하여 결과값을 도출함

 - 인공지능 프로그래밍 : (1)input 데이터를 먼저 넣고 그 후 output을 입력한 후 두 Data를 비교하여 규칙을 찾아냄(학습 과정)
                        (2) 학습과정 이후 : 제대로 Rule이 생겼는 지 확인하기 위해 new input을 입력하여 올바른 output이 나오는지 확인함(테스트과정)

4. 딥러닝과 머신러닝의 차이점은 무엇인가?
머신 러닝 : 컴퓨터가 명시적인 프로그래밍 없이도 데이터를 통해 스스로 학습하고 예측이나 판단하는 기술
 - 다양한 알고리즘 사용 (결정 트리, SVM 등)
 - 사람이 주요 특징(feature)을 수동으로 정의해야 함
 - 데이터 필요량 : 비교적 적은 양으로도 가능
 - 이메일 분류, 고객 이탈 예측 등
딥러닝 : 깊은 인공신경망 알고리즘을 활용하는 기술.
 - 인공 신경망을 깊게 쌓은 구조 사용
 - 모델이 자동으로 특징을 추출함
 - 매우 많은 데이터 필요
 - 이미지 인식, 음성 인식, 자연어 처리 등 복잡한 문제

5. Classification과 Regression의 주된 차이점은?
Classification(분류)
 - 분류는 이산적인 값, 즉 정해진 범주(label) 중 하나를 예측하는 방법
 - 레이블 사이에 연관이 있음(나이)
 - 정답이 명확하게 구분된 카테고리 중 하나를 선택하는 것
Regression(회귀)
 - 연속적인 값을 예측하는 방법
 - 레이블 사이의 연관성이 없음(성별)
 - 정답이 범주가 아니라 숫자 자체이기 때문에 오차를 줄이는 것이 핵심

6. 머신러닝에서 차원의 저주(curse of dimensionality)란?
차원의 저주는 공간의 차원이 증가할수록 데이터 분석과 머신러닝 모델이 학습하는 데 어려움을 겪는 현상을 의미한다.
차원이 높아지면 데이터의 분포, 거리 측정, 연산량 증가 등의 문제가 발생하며, 결과적으로 모델 성능이 저하될 수 있다.

7. Dimensionality Reduction는 왜 필요한가?
Dimensionality Reduction(차원 축소)는 데이터에서 불필요한 차원(특징)을 제거하거나 중요한 정보를 유지하면서 차원을 줄이는 과정이다.
차원이 많아지면(고차원 데이터) 차원의 저주(Curse of Dimensionality)가 발생하며, 머신러닝 모델의 성능이 저하될 수 있기 때문에 차원 축소가 필요하다.

차원 축소 방법
(1) 특징 선택 (Feature Selection)
가장 중요한 변수(feature)만 선택하여 차원을 줄이는 방법.
불필요한 변수를 제거하여 모델의 해석 가능성을 높임.
🔹 방법
 - 상관 계수(Feature Correlation): 높은 상관성을 가진 변수 중 하나만 선택
 - L1 정규화(Lasso Regression): 불필요한 특징을 자동으로 제거
 - 랜덤 포레스트 Feature Importance: 중요한 변수만 선택

(2) 특징 추출 (Feature Extraction)
원본 특징을 변형하여 새로운 저차원 특징을 생성하는 방법.
데이터의 구조를 유지하면서 정보량을 최대한 보존하는 것이 목표.
🔹 방법
 - PCA (Principal Component Analysis, 주성분 분석): 데이터를 가장 잘 설명하는 새로운 축(주성분)으로 변환
 - LDA (Linear Discriminant Analysis, 선형 판별 분석): 클래스 간 변별력을 높이면서 차원을 줄임
 - t-SNE, UMAP: 비선형 차원 축소 방법으로 시각화에 많이 사용됨
 - Autoencoder (오토인코더): 신경망을 이용한 차원 축소 및 데이터 압축

8. Ridge와 Lasso의 공통점과 차이점? (Regularization, 규제 , Scaling)
공통점 : 과적합을 막기 위한 규제 기법. 선형 회귀 모델에 자주 적용됨.
차이점 :
 - Ridge : 규제 방식 = L2 정규화(가중치의 제곱합), 대부분의 특성이 중요할 때 사용
 - Lasso : 규제 방식 = L1 정규화 (가중치의 절댓값 합), 소수의 중요한 특성만 쓸 때 사용

9. Overfitting vs. Underfitting
1. Overfitting (과적합)
(1) 정의
모델이 훈련 데이터에 지나치게 맞춰져서, 새로운 데이터(테스트 데이터)에서 성능이 떨어지는 현상.
즉, 훈련 데이터의 패턴뿐만 아니라, 노이즈까지 학습하여 일반화 성능이 낮아지는 문제.
(2) 특징
✅ 훈련 데이터에서 높은 정확도를 보이지만,
❌ 테스트 데이터(새로운 데이터)에서는 성능이 크게 하락함.
✅ 모델이 너무 복잡할 때 발생 (예: 너무 깊은 결정 트리, 과도한 다항식 회귀)
✅ 훈련 데이터의 작은 변화에도 예측값이 크게 변함
✅ 데이터에 존재하는 노이즈까지 학습하여 불필요한 패턴을 모델이 기억함.
(3) 원인
모델이 너무 복잡함 (파라미터가 너무 많음).
데이터가 부족한데 모델이 너무 많은 정보를 학습하려고 함.
학습 데이터에만 최적화된 모델을 만들었고, 일반적인 패턴을 학습하지 못함.
(4) 해결 방법
데이터를 더 많이 수집 (일반화 성능 증가)
특징 개수를 줄이거나(Feature Selection), 모델 단순화
정규화(Regularization) 적용 → Ridge(L2), Lasso(L1) 사용
Dropout 적용 (딥러닝 모델의 경우)
교차 검증(Cross-validation) 사용 → k-Fold Cross Validation 적용
(5) Overfitting 예제
예제 1: 과적합된 결정 트리
결정 트리(Decision Tree) 모델을 너무 깊게 만들면 훈련 데이터에 완벽하게 맞춤.
그러나 테스트 데이터에서는 일반화되지 않아 성능이 급격히 떨어짐.
예제 2: 다항 회귀(Polynomial Regression)
단순한 선형 회귀가 아니라, 다항식의 차수를 너무 높이면 훈련 데이터에 완전히 맞춰지지만 새로운 데이터에서는 잘 작동하지 않음.

2. Underfitting (과소적합)
(1) 정의
모델이 훈련 데이터의 패턴을 제대로 학습하지 못하여, 훈련 데이터와 테스트 데이터 모두에서 성능이 낮은 상태.
즉, 모델이 너무 단순해서 데이터의 패턴을 충분히 반영하지 못하는 문제.
(2) 특징
❌ 훈련 데이터와 테스트 데이터에서 모두 낮은 정확도를 보임.
❌ 패턴을 잘 학습하지 못하여, 예측 성능이 매우 낮음
✅ 모델이 너무 단순할 때 발생 (예: 선형 회귀를 사용했지만 실제 데이터는 비선형 관계)
(3) 원인
모델이 너무 단순함 (파라미터가 너무 적음).
충분한 학습을 하지 않음 (훈련 부족).
중요한 특징을 고려하지 않음 (Feature Selection이 부족함).
데이터 전처리 및 특성 엔지니어링이 부족함.
(4) 해결 방법
더 복잡한 모델 사용 (예: 선형 회귀 대신 다항 회귀 적용)
훈련 데이터 수 증가 (더 많은 데이터를 학습하여 패턴을 잘 학습하도록 함)
더 많은 Feature 추가 (중요한 변수 추가)
더 오래 학습 (학습 에포크(Epoch) 증가)
(5) Underfitting 예제
예제 1: 너무 단순한 선형 회귀
데이터가 곡선 형태인데, 선형 회귀(Linear Regression)만 사용하면 패턴을 제대로 반영하지 못함.
예제 2: 너무 얕은 신경망
딥러닝 모델에서 층이 너무 적거나 뉴런 수가 너무 적으면 패턴을 제대로 학습하지 못함.

10. Feature Engineering과 Feature Selection의 차이점은?
1. Feature Engineering (특징 엔지니어링)
(1) 정의
기존 데이터를 변형하거나 조합하여 새로운 특징(Feature)을 생성하는 과정.
모델이 데이터를 더 잘 이해하고 학습할 수 있도록 도와줌.
(2) 목적
✅ 모델이 더 나은 패턴을 학습하도록 도와줌
✅ 머신러닝 모델의 성능을 향상시킴
✅ 도메인 지식을 활용하여 의미 있는 특징을 추가함

(3) 방법
Feature Transformation (특징 변환)
데이터를 변형하여 새로운 정보를 추가하는 과정.
예: 로그 변환(Log Transformation), 제곱근 변환(Square Root Transformation)

Feature Creation (새로운 특징 생성)
기존 특징을 조합하여 새로운 변수를 만드는 과정.
예: 날짜 데이터 → "요일", "월", "연도" 등의 새로운 변수 생성

Encoding (범주형 데이터 변환)
범주형 데이터를 모델이 이해할 수 있는 숫자로 변환.
예: 원-핫 인코딩(One-Hot Encoding), 라벨 인코딩(Label Encoding)

Feature Scaling (특징 스케일링)
정규화(Normalization), 표준화(Standardization)를 적용하여 데이터 범위를 조정.

2. Feature Selection (특징 선택)
(1) 정의
기존 특징(Feature) 중에서 중요한 것만 선택하는 과정.
불필요한 특징을 제거하여 모델의 성능을 향상시킴.
(2) 목적
✅ 모델의 복잡도를 줄이고, 과적합(Overfitting)을 방지
✅ 연산 속도를 향상시키고 학습 시간을 단축
✅ 중요한 특징만 남겨 모델이 핵심 정보를 학습하도록 함

(3) 방법
Filter Method (필터 방식)
통계적 방법을 사용하여 중요한 변수를 선택.
예: 상관계수(Feature Correlation), 분산(Variance Threshold)
예제: 피어슨 상관계수를 이용해 불필요한 변수 제거

Wrapper Method (래퍼 방식)
모델을 직접 사용하여 중요한 특징을 선택하는 방법.
예: RFE (Recursive Feature Elimination), Forward Selection, Backward Elimination
예제: 랜덤 포레스트에서 중요한 변수 선택

Embedded Method (임베디드 방식)
모델이 학습하는 과정에서 자동으로 특징을 선택.
예: Lasso Regression (L1 정규화), Decision Tree 기반 Feature Importance
예제: Lasso 회귀를 적용하여 가중치가 0이 되는 변수를 제거

11. 전처리(Preprocessing)의 목적과 방법? (노이즈, 이상치, 결측치)
 - 전처리(Preprocessing)의 목적
데이터 품질 향상: 분석 및 모델 학습에 적합하도록 데이터를 정제.
모델 성능 향상: 노이즈, 이상치, 결측치를 제거해 정확도와 일반화 능력 향상.
학습 속도 개선: 불필요한 데이터 제거로 연산 자원 절약.
일관성 유지: 데이터 포맷과 스케일을 맞춰 모델이 데이터 구조를 이해하기 쉽게 함.

 - 전처리의 주요 방법
(1) 노이즈(Noise) 처리
목적: 의미 없는 데이터(오류, 무관한 값) 제거로 데이터 신뢰도 확보.
방법:
  평균/중앙값 필터링: 주변 값의 평균이나 중앙값으로 대체.
  스무딩(Smoothing): 데이터의 급격한 변동을 완화.
  이상치 탐지 후 제거: 노이즈가 이상치로 간주될 경우 함께 제거.
  정규표현식 활용: 텍스트 데이터에서 필요 없는 문자 제거.
(2) 이상치(Outlier) 처리
목적: 데이터 분포에서 벗어난 값 제거로 모델 왜곡 방지.
방법:
  통계적 방법: 평균, 표준편차로 범위 설정 (예: 3σ 밖의 값 제거).
  IQR(Interquartile Range): 사분위수를 이용해 이상치 범위 설정.
    IQR = Q3 - Q1,
    범위: [Q1 - 1.5IQR, Q3 + 1.5IQR]
  시각화: 박스플롯(Boxplot)으로 시각적으로 확인 후 제거.
  도메인 지식 기반: 해당 데이터의 합리적 범위를 알고 있다면 수동 제거.
(3) 결측치(Missing Values) 처리
목적: 비어 있는 데이터를 적절히 처리해 모델 학습에 문제 없게 만듦.
방법: 
  삭제: 결측치 비율이 높은 열/행 제거.
  대체(Imputation): 평균/중앙값/최빈값 대체: 수치형, 범주형 데이터에 따라 적용.
  예측 모델 대체: 다른 변수로 결측값 예측 (KNN, 회귀).
  고정값 대체: 'Unknown', 'None' 등으로 대체.
  보간법(Interpolation): 시간 순 데이터의 경우 선형 보간 등 활용.


12. EDA(Explorary Data Analysis)란? 데이터의 특성 파악(분포, 상관관계)
1. EDA(탐색적 데이터 분석)란?
데이터를 본격적으로 분석하거나 모델링하기 전, 데이터의 구조, 패턴, 특성, 분포, 상관관계 등을 파악하기 위한 과정.
데이터를 깊이 이해해서 이상치, 결측치, 변수 간 관계 등을 미리 확인함으로써, 적절한 전처리와 모델링 방향을 결정하는 데 도움.

2. EDA의 목적
데이터 이해:	데이터의 기본 구조, 변수 타입, 값의 범위, 요약 통계량 확인.
분포 확인:	변수들의 분포(정규성, 왜도, 첨도) 확인.
상관관계 파악:	변수 간 상관관계 및 패턴 파악.
이상치 탐색:	분포에서 벗어난 이상치 확인.
결측치 확인:	결측치가 있는 변수와 그 비율 확인.
가설 설정 및 검증:	데이터 기반으로 분석 가설 세우기.

13. 회귀에서 절편과 기울기가 의미하는 바는? 딥러닝과 어떻게 연관되는가?
1. 회귀에서 절편과 기울기의 의미
(1) 회귀 모델의 기본식
일반적인 선형 회귀(Linear Regression)는 다음과 같은 직선 방정식으로 표현돼.
y=wx+b
y: 예측값 (종속 변수, 타깃)
x: 입력값 (독립 변수, 피처)
w: 기울기(Slope) 또는 가중치(Weight)
b: 절편(Intercept) 또는 바이어스(Bias)
(2)기울기(Slope, w) 의미
입력값이 1만큼 증가할 때, 예측값이 얼마나 변하는지를 나타냄.
즉, x가 y에 미치는 영향의 크기와 방향.
w>0: x가 증가하면 y도 증가 (양의 상관관계).
w>0: x가 증가하면 y는 감소 (음의 상관관계).
w=0: x가 y에 영향을 주지 않음.
(3)절편(Intercept, b) 의미
x가 0일 때의 y값, 즉 시작점.
데이터의 기본적인 수준을 조정함 (기울기가 곱해지기 전에 y값을 얼마나 올려주거나 내려줄지 결정)

/* 14. Activation function 함수를 사용하는 이유? Softmax, Sigmoid 함수의 차이는?
15. Forward propagation, Backward propagation이란?
16. 손실함수란 무엇인가? 가장 많이 사용하는 손실함수 4가지 종류는?
17. 옵티마이저(optimizer)란 무엇일까? 옵티마이저와 손실함수의 차이점은?
18. 경사하강법 의미는? (확률적 경사하강법, 배치 경사하강법, 미치 배치경사하강법)
19. 교차검증, K-fold 교차검증의 의미와 차이
 - 교차검증
데이터를 여러 번 나눠가며 학습과 평가를 반복하는 방식.
모델이 우연히 훈련 데이터에만 잘 맞는 경우(과적합)를 막고, 일반화 성능을 더 정확히 측정할 수 있게 함.
하나의 고정된 학습/검증 분할이 아니라, 여러 번 다른 방식으로 나눠서 평가함
 - k-fold
 데이터를 K개의 폴드로 나누어, 각 폴드마다 한 번씩 검증 세트로 사용하고 나머지는 학습 세트로 사용하는 방법.
 이를 통해 K번의 학습과 검증을 수행하여, 모델의 평균 성능을 평가함.

20. 하이퍼파라미터 튜닝이란 무엇인가?
모델이 학습되기 전에 사람이 직접 설정해줘야 하는 값.
학습을 통해 자동으로 바뀌는 파라미터(가중치, 편향 등)와는 달리, 외부에서 설정해줘야 하는 값이기 때문에 따로 튜닝이 필요함.
그리드 서치 (Grid Search)	: 미리 정한 하이퍼파라미터 조합을 모두 탐색해서 최적 조합을 찾음 (완전탐색)
랜덤 서치 (Random Search)	: 일부 랜덤 조합만 무작위로 샘플링해서 비교 (시간 절약)
베이지안 최적화	: 이전 탐색 결과를 반영해 효율적으로 다음 조합을 추천함 (지능형 탐색)


21. CNN의 합성곱의 역활은?
22. CNN의 풀링층의 역활은?
23. CNN의 Dense Layer의 역활은?
24. CNN의 stride, filter의 역활? 필터의 가중치는 어떻게 결정되는가?
25. RNN을 사용하는 이유와 한계점은?
26. LSTM을 사용하는 이유와 한계점은?
27. GRU을 사용하는 이유와 차별성은? */

28. 결정트리에서 불순도(Impurity) - 지니 계수(Gini Index)란 무엇인가?
1. 불순도(Impurity)란?
한 노드(Node)에 섞여 있는 데이터의 혼합 정도를 의미.
즉, 한 노드에 여러 클래스(label)가 섞여 있을수록 불순도가 높음, 한 클래스만 있다면 불순도는 0 (완전 순수).
결정트리는 데이터를 순수한(한 가지 클래스만 있는) 노드로 분할하기 위해 불순도를 최소화하는 방향으로 나눔.
2. 지니 계수(Gini Index, Gini Impurity)란?
불순도를 측정하는 대표적인 지표.
노드 안의 데이터가 얼마나 섞여 있는지를 수치로 표현.
3. 결정트리에서 지니 계수의 역할
각 분할(split) 때, 지니 계수를 가장 낮출 수 있는 기준(feature, threshold)를 선택.
즉, 자식 노드들의 불순도를 최소화하도록 데이터를 나누는 것.
반복적으로 분할하여 불순도가 0에 가까운 순수한 노드를 만든다.

결정트리는 지니 계수를 최소화하면서 데이터 분할을 반복해서, 최종적으로 한 클래스만 남는 순수한 노드(leaf node)**를 만들어 예측하는 구조야.

29. 앙상블이란 무엇인가?
1. 앙상블(Ensemble)이란?
여러 개의 모델을 결합해서 하나의 더 좋은 모델을 만드는 기법.
각각의 모델이 가진 약점을 보완하고, 강점을 결합해서 더 정확하고 안정적인 예측을 목표로 함.
하나의 모델보다 성능이 뛰어난 경우가 많음.
2. 앙상블의 목적
성능: 향상	여러 모델의 예측을 결합해서 정확도, 정밀도, 재현율 향상.
과적합 방지(일반화 성능 향상):	개별 모델의 과적합(overfitting) 문제를 완화.
안정성 향상:	한 모델이 잘못 예측하더라도 다른 모델이 보완 가능.
3. 앙상블의 주요 방법 (종류)
  (1) 배깅(Bagging, Bootstrap Aggregating)
  여러 모델을 독립적으로 학습시켜 평균화 또는 투표로 예측.
  과적합 감소, 분산 줄이기에 효과적.
  대표 예시: 랜덤포레스트(Random Forest)
  특징: 데이터 샘플을 다르게 뽑아 여러 모델 훈련 → 결과를 평균/투표로 결정.
  (2) 부스팅(Boosting)
  약한 모델(weak learner)을 순차적으로 학습하면서 오류를 보완.
  이전 모델이 틀린 부분에 가중치를 두고 다음 모델이 더 잘 학습하도록 함.
  대표 예시: 그래디언트 부스팅(Gradient Boosting)
  XGBoost, LightGBM, CatBoost
  특징: 앞선 모델의 오차를 보완 → 성능 향상.
  (3) 스태킹(Stacking)
  여러 다른 종류의 모델을 조합.
  각각의 모델로 예측한 값을 다시 다른 모델(메타 모델, blender)에 학습시켜 최종 예측.
  예시: 로지스틱 회귀 + SVM + 결정트리 → 이들의 예측을 합쳐서 최종 결과 산출.
  특징: 다양한 모델 조합 → 복잡하고 강력한 예측.

30. 부트 스트랩핑(bootstraping)이란 무엇인가?
1. 부트스트래핑(Bootstrapping)이란?
하나의 데이터셋에서 여러 번 중복을 허용하여 샘플을 뽑는 방법.
즉, 복원 추출(With Replacement) 방식으로 데이터를 샘플링.
원본 데이터를 여러 개의 새로운 데이터셋(샘플)로 만들어, 이를 통해 통계적 추정이나 모델 학습을 진행.
2. 부트스트래핑의 목적
불확실성 추정:	표본 통계량(평균, 분산 등)의 분포를 추정. 신뢰구간 계산 가능.
모델 안정화:	작은 데이터로도 모델 성능을 안정적으로 평가.
앙상블 기법(Bagging 등)에 활용:	여러 샘플로 여러 모델을 훈련시켜 결합 (랜덤포레스트 등).
3. 부트스트래핑의 절차
(1)원본 데이터 n개 준비.
(2)중복을 허용해서 크기 n인 샘플 생성 (여러 번 가능).
(3)생성된 샘플로 통계량 계산 또는 모델 학습.
(4)여러 샘플의 결과를 결합해서 최종 통계나 예측 도출.

31. 배깅(Bagging)이란 무엇인가?
1. 배깅(Bagging)이란?
부트스트래핑(Bootstrapping)과 앙상블(Aggregating, 결합)을 활용한 머신러닝 기법.
여러 개의 모델을 독립적으로 학습한 뒤, 그 결과를 평균(회귀) 또는 투표(분류) 방식으로 결합해서 최종 예측을 만듦.
과적합 방지와 모델 성능 향상 효과가 있음.
4. 배깅의 장점과 단점
(1) 장점
과적합 방지:	여러 모델을 조합하기 때문에 개별 모델의 과적합이 줄어듦.
성능 향상:	개별 모델보다 안정적이고, 노이즈에 강함.
병렬 처리 가능:	모델이 독립적으로 학습되므로 병렬 연산 가능.
(2) 단점
메모리 사용량 증가:	여러 모델을 학습하므로 많은 연산이 필요함.
해석 어려움:	개별 모델은 직관적이지만, 결합된 결과는 복잡할 수 있음.

32. 주성분 분석(PCA) 이란 무엇인가?
1. 주성분 분석(PCA)란?
고차원 데이터를 저차원으로 축소(Dimensionality Reduction)하는 대표적인 기법.
데이터의 분산(정보)을 최대한 보존하면서 주요한 방향(축)을 찾아 새로운 축으로 데이터를 표현.
데이터에 포함된 중복 정보(상관관계)를 제거해서, 핵심적인 정보만 요약하는 방법.
2. PCA의 목적
차원 축소: 불필요한 변수를 제거하고, 데이터를 더 적은 수의 변수로 표현.
데이터 압축: 데이터의 중요한 정보만 남겨서 저장 공간 절약.
특징 추출: 데이터의 주요 특징을 추출하여, 모델의 성능을 향상시킬 수 있음.
시각화: 고차원 데이터를 2D, 3D로 줄여서 시각화.
노이즈 제거: 덜 중요한 변수를 제거하면서 데이터의 노이즈 완화.
3. PCA의 원리 (어떻게 동작하는가?)
(1) 데이터의 분산을 가장 잘 설명하는 방향 찾기
데이터에서 가장 많이 퍼져 있는 방향(분산이 큰 방향)을 찾아 첫 번째 주성분(PC1) 설정.
두 번째로 많이 퍼진 방향은 두 번째 주성분(PC2).
이렇게 해서 서로 수직(orthogonal, 독립)인 새로운 축(주성분)을 찾음.
(2) 변환과정
1. 표준화 : 데이터의 스케일을 맞추기 위해 평균 0, 분산 1로 정규화.
2. 공분산 행렬 : 변수 간 상관관계(공분산)를 나타내는 행렬 계산.
3. 고유값/고유벡터 : 공분산 행렬의 고유값(Eigenvalue), 고유벡터(Eigenvector) 계산.
4. 주성분 선택 : 고유값이 큰 순서대로 고유벡터 선택 (분산을 많이 설명하는 순서).
5. 데이터 변환 : 선택한 주성분으로 데이터를 변환 (투영).

// 33. Dense Layer란 무엇인가?

AI란 데이터의 특성을 잘 분석하는 기능

기존 프로그램
Data -> rule -> result 
input -> rule -> output

AI
Data -> result -> rule
1차(학습)
input(X_train), output(Y_train) -> rule
2차(테스트)
new input(X_test) -> output(Y_test)
 - train
 - test

Data 입력 방법
1. 파일로부터 받는 방법(2주차에 붓꽃데이터 파일로부터 Data입력받아서 각각 구분하는 코드 올리기. DT LR SVM RF)
2. 라이브러리로 부터 입력 받는 방법
3. 웹 주소로부터 입력 받는 방법

머신러닝
 - 어떤 특징을 추출하고 -> 특정 모델에 삽입

딥러닝
 - 더 낮은 데이터(low data)를 바로 추출 후 모델에 삽입(특징이 있는 데이터를 삽입하면 더 효율 좋음)